{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e11ace6-6ead-45f6-9950-0de2bd3988b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "import re\n",
    "import textwrap\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nbformat\n",
    "\n",
    "def print_table(section_title: str, rows: List[Tuple[str, str, float, str]]):\n",
    "    \"\"\"\n",
    "    Print a formatted table for a section.\n",
    "    \n",
    "    Args:\n",
    "        section_title (str): Title of the section to be displayed\n",
    "        rows (List[Tuple]): List of tuples containing (Detail, Marking, Mark, CodeDetail)\n",
    "    \n",
    "    Each row is formatted with wrapped text and proper alignment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define fixed column widths\n",
    "        col_detail = 60\n",
    "        col_marking = 12\n",
    "        col_mark = 8\n",
    "\n",
    "        # Prepare header and separator lines\n",
    "        header = f\"| {'Detail'.ljust(col_detail)} | {'Marking'.ljust(col_marking)} | {'Mark'.ljust(col_mark)} |\"\n",
    "        separator = f\"|{'-'*(col_detail+2)}|{'-'*(col_marking+2)}|{'-'*(col_mark+2)}|\"\n",
    "\n",
    "        print(f\"\\n--- {section_title.upper()} DETAILS ---\")\n",
    "        print(header)\n",
    "        print(separator)\n",
    "        \n",
    "        for detail, marking, mark, code_detail in rows:\n",
    "            mark_sym = \"✓\" if marking == \"Found\" else \"\"\n",
    "            detail_lines = textwrap.wrap(detail, width=col_detail) or [\"\"]\n",
    "            marking_lines = textwrap.wrap(marking, width=col_marking) or [\"\"]\n",
    "            mark_lines = textwrap.wrap(str(mark), width=col_mark) or [\"\"]\n",
    "            code_lines = textwrap.wrap(code_detail, width=col_detail) if code_detail.strip() else []\n",
    "            \n",
    "            max_lines = max(len(detail_lines), len(marking_lines), len(mark_lines))\n",
    "            \n",
    "            for i in range(max_lines):\n",
    "                d_line = detail_lines[i] if i < len(detail_lines) else \"\"\n",
    "                m_line = mark_sym.ljust(col_marking) if i == 0 else \"\".ljust(col_marking)\n",
    "                mark_line = mark_lines[i] if i < len(mark_lines) else \"\"\n",
    "                print(f\"| {d_line.ljust(col_detail)} | {m_line.ljust(col_marking)} | {mark_line.ljust(col_mark)} |\")\n",
    "            \n",
    "            for line in code_lines:\n",
    "                print(f\"| {'  ' + line.ljust(col_detail - 2)} | {' '.ljust(col_marking)} | {' '.ljust(col_mark)} |\")\n",
    "            print(separator)\n",
    "    except Exception as e:\n",
    "        print(f\"Error printing table: {str(e)}\")\n",
    "\n",
    "class NotebookGrader:\n",
    "    \"\"\"\n",
    "    A class to grade Jupyter notebooks based on specific criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the grader with default values and required imports.\"\"\"\n",
    "        self.total_score = 0\n",
    "        self.feedback = []\n",
    "        self.required_imports = ['numpy', 'pandas', 'matplotlib.pyplot', 'statsmodels.api']\n",
    "        self.details: Dict[str, List[Tuple[str, str, float, str]]] = {}\n",
    "        self.import_aliases = {\n",
    "            'numpy': ['np'],\n",
    "            'pandas': ['pd'],\n",
    "            'matplotlib.pyplot': ['plt'],\n",
    "            'statsmodels.api': ['sm']\n",
    "        }\n",
    "\n",
    "    def check_import_with_aliases(self, source: str, module: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a module is imported, including common aliases.\n",
    "        \n",
    "        Args:\n",
    "            source (str): Source code to check\n",
    "            module (str): Module name to look for\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if module or its alias is found\n",
    "        \"\"\"\n",
    "        patterns = [module] + self.import_aliases.get(module, [])\n",
    "        for pattern in patterns:\n",
    "            if re.search(rf\"^\\s*(import|from)\\s+{re.escape(pattern)}(\\W|$)\", source, re.MULTILINE | re.IGNORECASE):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def grade_imports(self, cells: List) -> int:\n",
    "        \"\"\"Grade the import section of the notebook.\"\"\"\n",
    "        try:\n",
    "            score = 0\n",
    "            found_imports = {}\n",
    "            \n",
    "            for req in self.required_imports:\n",
    "                code_detail = \"\"\n",
    "                for cell in cells:\n",
    "                    if cell['cell_type'] == 'code':\n",
    "                        if self.check_import_with_aliases(cell['source'], req):\n",
    "                            code_detail = next(line.strip() for line in cell['source'].splitlines() \n",
    "                                             if self.check_import_with_aliases(line, req))\n",
    "                            break\n",
    "                if code_detail:\n",
    "                    found_imports[req] = code_detail\n",
    "\n",
    "            rows = []\n",
    "            marks_per_import = 10 / len(self.required_imports)\n",
    "            for req in self.required_imports:\n",
    "                if req in found_imports:\n",
    "                    rows.append((f\"Found import: {req}\", \"Found\", marks_per_import, found_imports[req]))\n",
    "                else:\n",
    "                    rows.append((f\"Missing import: {req}\", \"Missing\", 0, \"\"))\n",
    "        \n",
    "            score = sum(mark for _, _, mark, _ in rows)\n",
    "            self.feedback.append(\"✓ All required imports present\" if score == 10 \n",
    "                               else f\"✗ Missing imports: {', '.join(req for req in self.required_imports if req not in found_imports)}\")\n",
    "            self.details['Imports'] = rows\n",
    "            return int(score)\n",
    "        except Exception as e:\n",
    "            self.feedback.append(f\"Error grading imports: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def grade_data_loading(self, cells: List) -> int:\n",
    "        score = 0\n",
    "        rows = []\n",
    "        found = False\n",
    "        # Loop over cells to check for pd.read_csv.\n",
    "        for i, cell in enumerate(cells):\n",
    "            if cell['cell_type'] == 'code':\n",
    "                detail = f\"Checking for pd.read_csv in cell {i+1}\"\n",
    "                if \"pd.read_csv\" in cell['source']:\n",
    "                    found = True\n",
    "                    code_detail = cell['source'].strip()\n",
    "                    rows.append((detail, \"Found\", 15, code_detail))\n",
    "                    break  # Stop after first found instance.\n",
    "                else:\n",
    "                    rows.append((detail, \"Missing\", 0, \"\"))\n",
    "    \n",
    "        if found:\n",
    "            score = 15\n",
    "            overall_feedback = \"✓ Data loading implemented correctly\"\n",
    "        else:\n",
    "            overall_feedback = \"✗ Missing or incorrect data loading\"\n",
    "    \n",
    "        self.feedback.append(overall_feedback)\n",
    "        self.details['Data Loading'] = rows\n",
    "        return score\n",
    "\n",
    "    def grade_simple_linear_regression(self, cells: List) -> int:\n",
    "        score = 0\n",
    "        required_elements = {'X1': None, 'X2': None, 'X3': None, 'X4': None, 'X5': None}\n",
    "        rows = []\n",
    "    \n",
    "        # For each variable, search for the corresponding regression model code.\n",
    "        for cell in cells:\n",
    "            if cell['cell_type'] == 'code':\n",
    "                source = cell['source']\n",
    "                for var in required_elements.keys():\n",
    "                    pattern = re.compile(rf\"lr_model_{var}\\s*=\\s*sm\\.OLS\\(\", re.IGNORECASE)\n",
    "                    if pattern.search(source) and required_elements[var] is None:\n",
    "                        required_elements[var] = source.strip()\n",
    "    \n",
    "        implemented_count = sum(1 for v in required_elements.values() if v is not None)\n",
    "        score = int((implemented_count / 5) * 25)\n",
    "    \n",
    "        for var, code_detail in required_elements.items():\n",
    "            if code_detail:\n",
    "                rows.append((f\"Found linear regression model for {var}\", \"Found\", 25/5, code_detail))\n",
    "            else:\n",
    "                rows.append((f\"Missing linear regression model for {var}\", \"Missing\", 0, \"\"))\n",
    "    \n",
    "        if implemented_count == 5:\n",
    "            overall_feedback = \"✓ All simple linear regressions implemented correctly\"\n",
    "        else:\n",
    "            overall_feedback = \"✗ Missing some linear regression implementations\"\n",
    "    \n",
    "        self.feedback.append(overall_feedback)\n",
    "        self.details['Simple Linear Regression'] = rows\n",
    "        return score\n",
    "\n",
    "    def grade_scatter_plots(self, cells: List) -> int:\n",
    "        score = 0\n",
    "        plot_count = 0\n",
    "        rows = []\n",
    "        # Check each cell for both plt.scatter and plt.plot.\n",
    "        for i, cell in enumerate(cells):\n",
    "            if cell['cell_type'] == 'code':\n",
    "                detail = f\"Checking scatter plot in cell {i+1}\"\n",
    "                if 'plt.scatter' in cell['source'] and 'plt.plot' in cell['source']:\n",
    "                    plot_count += 1\n",
    "                    code_detail = cell['source'].strip()\n",
    "                    rows.append((detail, \"Found\", 7, code_detail))\n",
    "                else:\n",
    "                    rows.append((detail, \"Missing\", 0, \"\"))\n",
    "    \n",
    "        score = min(plot_count * 7, 20)\n",
    "        if score == 20:\n",
    "            overall_feedback = \"✓ All scatter plots implemented correctly\"\n",
    "        else:\n",
    "            overall_feedback = \"✗ Missing or incomplete scatter plots\"\n",
    "    \n",
    "        self.feedback.append(overall_feedback)\n",
    "        self.details['Scatter Plots'] = rows\n",
    "        return score\n",
    "\n",
    "    def grade_multiple_regression(self, cells: List) -> int:\n",
    "        score = 0\n",
    "        found_multiple = None\n",
    "        found_prediction = None\n",
    "        rows = []\n",
    "    \n",
    "        # Search cells for multiple regression and prediction.\n",
    "        for i, cell in enumerate(cells):\n",
    "            if cell['cell_type'] == 'code':\n",
    "                source = cell['source']\n",
    "                if (not found_multiple and \n",
    "                    re.search(r\"(linear|lr|regression)_model_(MultipleR|MultiR|Multiple_Regression)\\s*=\\s*sm\\.OLS\\(\", \n",
    "                              source, re.IGNORECASE)):\n",
    "                    found_multiple = source.strip()\n",
    "                    rows.append((f\"Checking for multiple regression in cell {i+1}\", \"Found\", 10, found_multiple))\n",
    "                if (not found_prediction and 'Performance=' in source):\n",
    "                    found_prediction = source.strip()\n",
    "                    rows.append((f\"Checking for prediction in cell {i+1}\", \"Found\", 10, found_prediction))\n",
    "    \n",
    "        if found_multiple and found_prediction:\n",
    "            score = 20\n",
    "            overall_feedback = \"✓ Multiple regression implemented correctly\"\n",
    "        else:\n",
    "            overall_feedback = \"✗ Issues with multiple regression implementation\"\n",
    "    \n",
    "        self.feedback.append(overall_feedback)\n",
    "        self.details['Multiple Regression'] = rows\n",
    "        return score\n",
    "\n",
    "    def grade_reasoning(self, cells: List) -> int:\n",
    "        score = 0\n",
    "        rows = []\n",
    "        reasoning_keywords = [\"difference\", \"close\", \"error\", \"r-square\"]\n",
    "        required_values = [\"80.28\", \"76\"]\n",
    "        found_values = False\n",
    "        found_reasoning = False\n",
    "    \n",
    "        for i, cell in enumerate(cells):\n",
    "            if cell['cell_type'] == 'markdown':\n",
    "                source_lower = cell['source'].lower()\n",
    "                detail = f\"Checking reasoning in markdown cell {i+1}\"\n",
    "                if all(val in source_lower for val in required_values):\n",
    "                    found_values = True\n",
    "                    rows.append((f\"{detail}: Found required values\", \"Found\", 5, cell['source'].strip()))\n",
    "                else:\n",
    "                    rows.append((f\"{detail}: Required values not found\", \"Missing\", 0, \"\"))\n",
    "    \n",
    "                if found_values and any(keyword in source_lower for keyword in reasoning_keywords):\n",
    "                    found_reasoning = True\n",
    "                    rows.append((f\"{detail}: Found reasoning keywords\", \"Found\", 5, cell['source'].strip()))\n",
    "                    break\n",
    "    \n",
    "        if found_reasoning:\n",
    "            score = 10\n",
    "            overall_feedback = \"✓ Reasoning for actual vs predicted values provided\"\n",
    "        else:\n",
    "            overall_feedback = \"✗ Missing or incomplete reasoning for actual vs predicted values\"\n",
    "    \n",
    "        self.feedback.append(overall_feedback)\n",
    "        self.details['Reasoning'] = rows\n",
    "        return score\n",
    "\n",
    "    def grade_notebook(self, notebook_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Grade a Jupyter notebook file.\n",
    "        \n",
    "        Args:\n",
    "            notebook_path (str): Path to the notebook file\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Grading results including scores and feedback\n",
    "        \"\"\"\n",
    "        try:\n",
    "            notebook_path = Path(notebook_path)\n",
    "            if not notebook_path.exists():\n",
    "                raise FileNotFoundError(f\"Notebook file not found: {notebook_path}\")\n",
    "\n",
    "            with open(notebook_path) as f:\n",
    "                nb = nbformat.read(f, as_version=4)\n",
    "            cells = nb['cells']\n",
    "        \n",
    "            scores = {\n",
    "                'imports': self.grade_imports(cells),\n",
    "                'data_loading': self.grade_data_loading(cells),\n",
    "                'simple_linear_regression': self.grade_simple_linear_regression(cells),\n",
    "                'scatter_plots': self.grade_scatter_plots(cells),\n",
    "                'multiple_regression': self.grade_multiple_regression(cells),\n",
    "                'reasoning': self.grade_reasoning(cells)\n",
    "            }\n",
    "        \n",
    "            self.total_score = sum(scores.values())\n",
    "        \n",
    "            return {\n",
    "                'total_score': self.total_score,\n",
    "                'percentage': (self.total_score / 100) * 100,\n",
    "                'component_scores': scores,\n",
    "                'feedback': self.feedback,\n",
    "                'details': self.details\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'total_score': 0,\n",
    "                'percentage': 0,\n",
    "                'component_scores': {},\n",
    "                'feedback': [f\"Error grading notebook: {str(e)}\"],\n",
    "                'details': {}\n",
    "            }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b413dd-9711-4e62-9415-91c9be0a72c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: class1_1132.ipynb\n",
      "\n",
      "Processing: class1_1133.ipynb\n",
      "\n",
      "Processing: class2_1234.ipynb\n",
      "\n",
      "Processing: class2_1263.ipynb\n"
     ]
    }
   ],
   "source": [
    "def extract_class_and_id(filename: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Extract class name and student ID from filename.\n",
    "    Expected format: class#_####.ipynb (e.g., class1_1234.ipynb)\n",
    "    \"\"\"\n",
    "    match = re.match(r'(class\\d+)_(\\d+)\\.ipynb', filename)\n",
    "    if match:\n",
    "        return match.group(1), match.group(2)\n",
    "    return \"unknown_class\", \"unknown_id\"\n",
    "\n",
    "def print_detailed_student_report(f, results: Dict[str, Any], student_id: str):\n",
    "    \"\"\"Print detailed student report to file.\"\"\"\n",
    "    f.write(f\"\\n{'='*80}\\n\")\n",
    "    f.write(f\"DETAILED GRADING REPORT FOR STUDENT {student_id}\\n\")\n",
    "    f.write(f\"{'='*80}\\n\\n\")\n",
    "\n",
    "    f.write(f\"OVERALL SCORE: {results['total_score']}/100 ({results['percentage']}%)\\n\")\n",
    "    f.write(f\"{'-'*80}\\n\\n\")\n",
    "\n",
    "    f.write(\"COMPONENT SCORES:\\n\")\n",
    "    f.write(f\"{'-'*40}\\n\")\n",
    "    for component, score in results['component_scores'].items():\n",
    "        f.write(f\"{component.replace('_', ' ').title():30}: {score}/20\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"DETAILED FEEDBACK:\\n\")\n",
    "    f.write(f\"{'-'*40}\\n\")\n",
    "    for fb in results['feedback']:\n",
    "        f.write(f\"• {fb}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"DETAILED ANALYSIS BY SECTION:\\n\")\n",
    "    f.write(f\"{'-'*40}\\n\")\n",
    "    for section, rows in results['details'].items():\n",
    "        f.write(f\"\\n{section.upper()}:\\n\")\n",
    "        f.write(f\"{'-'*40}\\n\")\n",
    "        \n",
    "        for detail, marking, mark, code_detail in rows:\n",
    "            f.write(f\"Check: {detail}\\n\")\n",
    "            f.write(f\"Status: {marking}\\n\")\n",
    "            f.write(f\"Points: {mark}\\n\")\n",
    "            if code_detail.strip():\n",
    "                f.write(f\"Code: {code_detail}\\n\")\n",
    "            f.write(f\"{'-'*20}\\n\")\n",
    "\n",
    "def generate_class_visualizations(df: pd.DataFrame, class_name: str, class_dir: Path):\n",
    "    \"\"\"Generate visualizations for class performance\"\"\"\n",
    "    try:\n",
    "        # Score distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=df, x='Total_Score', bins=10)\n",
    "        plt.title(f'Score Distribution - {class_name}')\n",
    "        plt.savefig(class_dir / f\"{class_name}_score_distribution.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Component scores\n",
    "        component_columns = [col for col in df.columns \n",
    "                           if col not in ['Student_ID', 'Total_Score', 'Percentage']]\n",
    "        if component_columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            df[component_columns].boxplot()\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.title(f'Component Scores Distribution - {class_name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(class_dir / f\"{class_name}_component_scores.png\")\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating visualizations: {str(e)}\")\n",
    "\n",
    "def process_notebooks(folder_path: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Process notebooks with detailed reports organized by class.\"\"\"\n",
    "    reports_dir = Path(folder_path) / \"grading_reports\"\n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    class_results = {}\n",
    "    \n",
    "    for notebook_file in Path(folder_path).glob(\"*.ipynb\"):\n",
    "        try:\n",
    "            if notebook_file.name.startswith('.') or 'checkpoint' in notebook_file.name:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nProcessing: {notebook_file.name}\")\n",
    "            \n",
    "            class_name, student_id = extract_class_and_id(notebook_file.name)\n",
    "            \n",
    "            # Create class directory and its subdirectories\n",
    "            class_dir = reports_dir / class_name\n",
    "            class_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            detailed_reports_dir = class_dir / \"detailed_reports\"\n",
    "            summary_reports_dir = class_dir / \"summary_reports\"\n",
    "            detailed_reports_dir.mkdir(exist_ok=True)\n",
    "            summary_reports_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            grader = NotebookGrader()\n",
    "            results = grader.grade_notebook(str(notebook_file))\n",
    "            \n",
    "            # Generate detailed student report\n",
    "            detailed_report_file = detailed_reports_dir / f\"detailed_report_student_{student_id}.txt\"\n",
    "            with open(detailed_report_file, 'w', encoding='utf-8') as f:\n",
    "                print_detailed_student_report(f, results, student_id)\n",
    "            \n",
    "            # Generate summary report\n",
    "            summary_report_file = summary_reports_dir / f\"summary_student_{student_id}.txt\"\n",
    "            with open(summary_report_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Summary Report for Student {student_id}\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\")\n",
    "                f.write(f\"Total Score: {results['total_score']}/100\\n\")\n",
    "                f.write(f\"Percentage: {results['percentage']}%\\n\\n\")\n",
    "                f.write(\"Component Scores:\\n\")\n",
    "                for component, score in results['component_scores'].items():\n",
    "                    f.write(f\"{component}: {score}\\n\")\n",
    "            \n",
    "            result_dict = {\n",
    "                'Student_ID': student_id,\n",
    "                'Total_Score': results['total_score'],\n",
    "                'Percentage': results['percentage'],\n",
    "                **results['component_scores']\n",
    "            }\n",
    "            \n",
    "            if class_name not in class_results:\n",
    "                class_results[class_name] = []\n",
    "            class_results[class_name].append(result_dict)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {notebook_file.name}: {str(e)}\")\n",
    "            if class_name not in class_results:\n",
    "                class_results[class_name] = []\n",
    "            class_results[class_name].append({\n",
    "                'Student_ID': student_id,\n",
    "                'Total_Score': 0,\n",
    "                'Percentage': 0,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "\n",
    "    class_dataframes = {}\n",
    "    \n",
    "    for class_name, results in class_results.items():\n",
    "        df = pd.DataFrame(results)\n",
    "        class_dataframes[class_name] = df\n",
    "        \n",
    "        class_dir = reports_dir / class_name\n",
    "        \n",
    "        # Generate class summary\n",
    "        with open(class_dir / f\"{class_name}_class_summary.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Class Summary Report for {class_name}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"Class Statistics:\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Total Students: {len(df)}\\n\")\n",
    "            f.write(f\"Average Score: {df['Total_Score'].mean():.2f}\\n\")\n",
    "            f.write(f\"Highest Score: {df['Total_Score'].max():.2f}\\n\")\n",
    "            f.write(f\"Lowest Score: {df['Total_Score'].min():.2f}\\n\")\n",
    "            f.write(f\"Standard Deviation: {df['Total_Score'].std():.2f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Score Distribution:\\n\")\n",
    "            f.write(df['Total_Score'].describe().to_string())\n",
    "            f.write(\"\\n\\nDetailed Grades:\\n\")\n",
    "            f.write(df.to_string())\n",
    "        \n",
    "        # Generate Excel report\n",
    "        df.to_excel(class_dir / f\"{class_name}_grades.xlsx\", index=False)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        generate_class_visualizations(df, class_name, class_dir)\n",
    "    \n",
    "    # Generate overall summary\n",
    "    with open(reports_dir / \"overall_summary.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"OVERALL GRADING SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        all_scores = pd.concat([df['Total_Score'] for df in class_dataframes.values()])\n",
    "        \n",
    "        f.write(\"Overall Statistics:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        f.write(f\"Total Students: {len(all_scores)}\\n\")\n",
    "        f.write(f\"Overall Average: {all_scores.mean():.2f}\\n\")\n",
    "        f.write(f\"Overall Highest: {all_scores.max():.2f}\\n\")\n",
    "        f.write(f\"Overall Lowest: {all_scores.min():.2f}\\n\\n\")\n",
    "        \n",
    "        for class_name, df in class_dataframes.items():\n",
    "            f.write(f\"\\nClass: {class_name}\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Students: {len(df)}\\n\")\n",
    "            f.write(f\"Average: {df['Total_Score'].mean():.2f}\\n\")\n",
    "            f.write(f\"Highest: {df['Total_Score'].max():.2f}\\n\")\n",
    "            f.write(f\"Lowest: {df['Total_Score'].min():.2f}\\n\")\n",
    "    \n",
    "    return class_dataframes\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"C:\\Users\\lewka\\Downloads\\quiz\"  # Replace with your folder path\n",
    "    results = process_notebooks(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8306d-b3e2-4092-9c54-cf6cf801f4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
